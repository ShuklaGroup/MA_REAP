import os
from collections import Counter

import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from utils import save_pickle, run_monte_carlo_traj, num_clusters, \
    spawn_trajectories_MA


def clustering_MA(state_sequences, n_agents, mapping, n_clusters=None, max_frames=1e5):
    '''
    Clusters all (or a representative subset) of the frames in trajectories using KMeans. Returns clustering object, which will be used to select the 
    least counts clusters. The agents that created each frame are remembered and their indices are returned as well. The selected subset of the total
    frames are also returned.
    
    Args
    -------------
    state_sequences (list[list[np.ndarray]]): trajectories collected so far (in terms of MSM states indices). 
        They should be accessed as trajectories[ith_agent][jth_trajectory].
    n_agents (int): number of agents.
    n_clusters (int), default None: number of clusters to use for KMeans. If None, a heuristic is used to approximate the number of clusters needed.
    max_frames (int), default 1e5: maximum number of frames to use in the clustering step. If set to 0 or None, all frames are used.
    
    Returns
    -------------
    KMeans (sklearn.cluster.KMeans): fitted KMeans clustering object.
    X (np.ndarray): array of shape (max_frames,) containing the subset of the data (in terms of MSM states indices) used for clustering.
    agent_idx (np.ndarray): array of shape (max_frames,) containing the index of the agent that originated each frame.
    '''

    # Put frames in format that is usable for KMeans
    assert (n_agents == len(state_sequences))
    total_frames = 0
    trajectory = []  # All frames
    agent_index = []  # Array mapping a frame index in trajectory to its corresponding agent
    for a, agent_trajs in enumerate(state_sequences):
        for traj in agent_trajs:
            total_frames += len(traj)
            trajectory.append(traj)
            agent_index.extend([a] * len(traj))

    trajectory = np.concatenate(trajectory)
    agent_index = np.asarray(agent_index)

    # Downsample number of points
    if (not max_frames) or (total_frames <= max_frames):
        X = trajectory
        agent_idx = agent_index

    elif total_frames > max_frames:
        max_frames = int(max_frames)
        rng = np.random.default_rng()
        rand_indices = rng.choice(len(trajectory), max_frames, replace=False)
        X = trajectory[rand_indices]
        agent_idx = agent_index[rand_indices]

    # Use n_clusters = number of states discovered (and present in random sample)
    if (n_clusters is None):
        n_clusters = num_clusters(X)

    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=3).fit(mapping(X))

    return kmeans, X, agent_idx


def select_least_counts_MA(kmeans, X, agent_idx, n_agents, mapping, n_select=50, stakes_method='percentage',
                           stakes_k=None):
    '''
    Select candidate clusters for new round of simulations based on least counts policy.
    
    Args
    -------------
    kmeans (sklearn.cluster.KMeans): KMeans clustering object fitted on X.
    X (np.ndarray): array of shape (max_frames,) containing the subset of the data (in terms of MSM states indices) used for clustering.
    agent_idx (np.ndarray): array of shape (n_frames,) indicating which agent originated each frame.
    n_agents (int): number of agents.
    n_select (int), default 50: how many candidates to select based on least counts policy.
    
    Returns
    -------------
    central_frames (np.ndarray): array of shape (n_select,). Frames in X that are closest to the center of each candidate (in terms of MSM states).
    central_frames_indices (np.ndarray): array of shape (n_select,). Indices of the frames in X that are closest to the center of each candidate.
    agent_stakes (np.ndarray): array of shape (n_agents, n_select). Entry agent_stakes[i, j] indicates the fraction of frames from 
        candidate j that were generated by agent i.
    '''

    # Select n_select candidates via least counts
    counts = Counter(kmeans.labels_)
    least_counts = np.asarray(counts.most_common()[::-1][:n_select])[:, 0]  # Which clusters contain lowest populations

    # Find frames closest to cluster centers of candidates
    least_counts_centers = kmeans.cluster_centers_[least_counts]
    central_frames_indices, _ = pairwise_distances_argmin_min(least_counts_centers, mapping(X))
    central_frames = X[central_frames_indices]

    # Compute agent stakes
    agent_stakes_raw = np.zeros((n_agents, len(central_frames)))
    for candidate_idx, candidate in enumerate(least_counts):
        agent_indices = agent_idx[np.where(kmeans.labels_ == candidate)]
        total = len(agent_indices)
        agent_indices_count = Counter(agent_indices).most_common()
        for agent, count in agent_indices_count:
            agent_stakes_raw[agent, candidate_idx] = count / total

    agent_stakes = compute_agent_stakes(agent_stakes_raw, stakes_method, k=stakes_k)

    return central_frames, central_frames_indices, agent_stakes


def compute_agent_stakes(agent_stakes_raw, method='percentage', k=None):
    '''
    Returns agent stakes of a cluster given the number of frames from each agent that fall in said cluster.
    
    Args
    -------------
    agent_stakes_raw (np.ndarray): array of shape (n_agents, n_clusters). Stakes if method used is percentage. 
    method (str): one of "percentage", "max", "equal", or "logistic". 
        percentage: stakes are proportional to number of frames that agents have in the cluster.
        max: the agent with the max number of frames in the cluster has all the stakes.
        equal: all agents with at least one frame in the cluster have the same stakes.
        logistic: percentage stakes are transformed as 1/(1+e^(-k(x-0.5))) and renormalized. Must set parameter k (higher k ~= max, lower k ~= equal).
    k (float): parameter for logistic method. Ignored if method is not set to logistic.
    
    Returns
    -------------
    agent_stakes (np.ndarray): array of shape (n_agents, n_clusters). Entry agent_stakes[i, j] indicates the stakes that agent i has on cluster j.
    '''

    if method == 'percentage':
        return agent_stakes_raw

    elif method == 'max':
        agent_stakes = np.empty(agent_stakes_raw.shape)
        for n in range(agent_stakes_raw.shape[1]):
            agent_stakes[:, n] = (agent_stakes_raw[:, n] == agent_stakes_raw[:, n].max()).astype(int)
        return agent_stakes

    elif method == 'equal':
        agent_stakes = np.empty(agent_stakes_raw.shape)
        for n in range(agent_stakes_raw.shape[1]):
            agent_stakes[:, n][np.where(agent_stakes_raw[:, n] != 0)] = 1 / np.count_nonzero(agent_stakes_raw[:, n])
        return agent_stakes

    elif method == 'logistic':
        if k is None:
            raise ValueError('k must be specified if using method logistic')

        x0 = 0.5
        logistic_fun = lambda x: 1 / (1 + np.exp(-k * (x - x0)))

        agent_stakes = np.empty(agent_stakes_raw.shape)
        for n in range(agent_stakes_raw.shape[1]):
            stakes_transformed = logistic_fun(agent_stakes_raw[:, n])
            stakes_transformed[
                np.where(agent_stakes_raw[:, n] < 1e-18)] = 0  # Make sure that the function evaluates to 0 at x=0
            stakes_transformed /= stakes_transformed.sum()  # Re-normalize
            agent_stakes[:, n] = stakes_transformed

        return agent_stakes

    else:
        raise ValueError(
            "Method " + method + " not understood. Must choose 'percentage', 'max', 'equal', or 'logistic'.")


def compute_cumulative_reward_MA_standard_Euclidean(X, agent_idx, agent_stakes, central_frames_indices, n_select,
                                                    n_agents, weights, which_agent, mapping):
    '''
    Returns the cumulative reward for current weights and a callable to the cumulative reward function (necessary to finetune weights).
    Note that this is the cumulative reward and reward function for the given agent.
    In the multi-agent implementation, this function is called n_agents times during the optimization step.
    
    Args
    -------------
    X (np.ndarray): array of shape (n_frames,). Representative subset of frames from trajectories (in terms of MSM states indices).
    agent_idx (np.ndarray): array of shape (n_frames,) indicating which agent originated each frame.
    agent_stakes (np.ndarray): array of shape (n_agents, n_select). Entry agent_stakes[i, j] indicates the fraction of frames from 
        candidate j that were generated by agent i.
    central_frames_indices (np.ndarray): array of shape (n_select,). Indices of the frames in X that are closest to the center of each candidate.
    n_select (int): how many candidates were selected based on least counts policy.
    n_agents (int): number of agents.
    weights (np.ndarray): array of shape (n_agents, n_features). Weights that each agent assigns to each order parameter.
    which_agent (int): number in [0, n_agents). Indicates which agent is computing the rewards.
    mapping (callable): map from MSM state to CV space.
    
    Returns
    -------------
    R (float): cumulative reward for selected agent.
    rewards_function (callable): reward function to maximize.  
    '''
    assert (which_agent < n_agents)

    # Access relevant frames
    central_frames = X[central_frames_indices]

    # Acess data for specific agent
    a = which_agent
    weights_a = weights[a]
    stakes_a = agent_stakes[a]

    indices = np.where(agent_idx == a)
    X_a = X[indices]

    def rewards_function(w):
        r = compute_structure_reward_MA_standard_Euclidean(mapping(X_a),
                                                           mapping(central_frames),
                                                           stakes_a, w, n_select)
        R = r.sum()
        return R

    R = rewards_function(weights_a)

    return R, rewards_function


def compute_structure_reward_MA_standard_Euclidean(X_a, central_frames, stakes_a, weights_a, n_select):
    '''
    Computes the reward for each structure w.r.t. the given agent and returns it as an array of shape (n_select,).
    
    Args
    -------------
    X_a (np.ndarray): array of shape (?, n_features). Frames observed by a given agent (in CV space).
    central_frames (np.ndarray): array of shape (n_select, n_features). Frames in X that are closest to the center of each candidate.
    stakes_a (np.ndarray): array of shape (n_select,). Stakes of the agent in each candidate.
    weights_a (np.ndarray): array of shape (n_features,). Weights that the given agent assigns to each order parameter.
    n_select (int): how many candidates were selected based on least counts policy.
    
    Returns
    -------------
    rewards (np.ndarray): array of shape (n_select,). Rewards assigned to candidates by given agent.
    '''
    # Initialize array
    rewards = np.empty(central_frames.shape)

    # Compute distribution parameters for frames observed by the agent
    mu = X_a.mean(axis=0)
    sigma = X_a.std(axis=0)

    # Compute distance of each candidate to the mean
    distances = (weights_a * np.abs(central_frames - mu) / sigma).sum(axis=1)  # Shape is (n_select,)

    # Compute rewards
    # Since stakes_a is zero for those clusters that do not involve agent a, rewards are accurately set to zero here.
    rewards = stakes_a * distances

    return rewards


def tune_weights_MA_standard_Euclidean(rewards_function, weights_a, delta=0.02):
    '''
    Defines constraints for optimization and maximizes rewards function. Returns OptimizeResult object.
    This function is called once per each agent per epoch.
    
    Args
    -------------
    rewards_function (callable): reward function to maximize. This corresponds to a given agent.
    weights_a (np.ndarray): array of shape (n_features,). Weights that the given agent assigns to each order parameter.
    delta (float): maximum amount by which an entry in the weights matrix can change. Think of it as an upper-bounded learning rate.
    
    Returns
    -------------
    results (scipy.optimize.OptimizeResult): result of the optimization for the weights of the given agent.
    '''
    weights_prev = weights_a

    # Create constraints
    constraints = []

    # Inequality constraints (fun(x, *args) >= 0)
    # This constraint makes the weights change by delta (at most)
    constraints.append({
        'type': 'ineq',
        'fun': lambda weights, weights_prev, delta: delta - np.abs((weights_prev - weights)),
        'jac': lambda weights, weights_prev, delta: np.diagflat(np.sign(weights_prev - weights)),
        'args': (weights_prev, delta),
    })

    # This constraint makes the weights be always positive
    constraints.append({
        'type': 'ineq',
        'fun': lambda weights: weights,
        'jac': lambda weights: np.eye(weights.shape[0]),
    })

    # Equality constraints (fun(x, *args) = 0)
    # This constraint makes sure the weights add up to one
    constraints.append({
        'type': 'eq',
        'fun': lambda weights: weights.sum() - 1,
        'jac': lambda weights: np.ones(weights.shape[0]),
    })

    results = minimize(lambda x: -rewards_function(x), weights_prev, method='SLSQP', constraints=constraints)

    return results


def select_starting_points_MA_standard_Euclidean(X, central_frames, central_frames_indices, agent_idx, agent_stakes,
                                                 weights, n_agents, mapping, n_chosen=10):
    '''
    Select starting positions for new simulations. Here, the collective rewards are computed as the sum of the rewards from each agent.
    
    Args
    -------------
    X (np.ndarray): array of shape (n_frames,). Representative subset of frames from trajectories (in terms of MSM states indices).
    central_frames (np.ndarray): array of shape (n_select,). Frames that are closest to the cluster center of each candidate (in terms of MSM states indices).
    central_frames_indices (np.ndarray): array of shape (n_select,). Indices of the frames in X that are closest to the center of each candidate.
    agent_idx (np.ndarray): array of shape (n_frames,) indicating which agent originated each frame.
    agent_stakes (np.ndarray): array of shape (n_agents, n_select). Entry agent_stakes[i, j] indicates the fraction of frames from 
        candidate j that were generated by agent i.
    weights (list[np.ndarray]): list of length n_agents of shape (n_features, n_features). Weights that each agent assigns to each entry in the
        order parameter covariance matrix.
    n_agents (int): number of agents.
    mapping (callable): map from MSM state to CV space.
    n_chosen (int): number of candidates that will be used for new trajectories.
    
    Returns
    -------------
    chosen_frames (np.ndarray): array of shape (n_chosen,). Frames from which to launch new trajectories (in terms of MSM states).
    executors (np.ndarray): array of shape (n_chosen,). Index of the agent that launches each trajectory.
    '''

    # Compute collective rewards for each candidate with the updated weights
    n_select = len(central_frames)
    rewards = np.zeros(n_select)

    for a in range(n_agents):
        # Acess data for specific agent
        weights_a = weights[a]
        stakes_a = agent_stakes[a]

        indices = np.where(agent_idx == a)
        X_a = X[indices]
        rewards += compute_structure_reward_MA_standard_Euclidean(mapping(X_a),
                                                                  mapping(central_frames),
                                                                  stakes_a, weights_a, n_select)

    indices = np.argsort(rewards)[-n_chosen:][::-1]  # Indices of frames with maximum reward
    chosen_frames = central_frames[indices]  # Frames that will be used to start new simulations
    chosen_frames_indices = central_frames_indices[indices]  # Frame indices that will be used to start new simulations
    executors = agent_stakes.argmax(axis=0)[indices]  # Agents that will run the new trajectories

    return chosen_frames, chosen_frames_indices, executors


def run_trial(msm, initial_states, epochs, output_dir='./', output_prefix='', **kwargs):
    '''
    Runs a trial of MA REAP with standard Euclidean distance rewards.
    
    Args
    -------------
    msm (pyemma.msm.models.msm.MSM): Markov State Model that will determine the kinetics of the simulation.
    initial_states (list[int]): starting states for simulations. Lenght of list must match number of agents.
    epochs (int): specifies for how many epochs to run a trial.
    output_dir (str): folder where to store the results (it will be created in current working directory if it does not exist).
    output_prefix (str): common prefix for all log files.
    **kwargs: used to specify model hyperparameters. Must include following keys:
        num_spawn (int): number of total trajectories to spawn per epoch.
        n_select (int): number of least-count candidates selected per epoch.
        n_agents (int): number of agents.
        traj_len (int): length of each trajectory ran.
        delta (float): upper boundary for learning step.
        n_features (int): number of collective variables.
        max_frames (int): maximum number of frames to use in clustering steps (take random subsample to accelerate testing).
        mapping (callable): mapping from MSM state to CV space.
        stakes_method (str): method used to compute agent stakes.
        stakes_k (float): k parameter for logistic stakes.
    
    Returns
    -------------
    None. Results are saved to output_dir.
    '''

    # Create directory
    if not os.path.isdir(output_dir):
        os.mkdir(output_dir)

    # Step 1: define some hyperparameters and initialize arrays
    num_spawn = kwargs['num_spawn']  # Number of trajectories spawn per epoch
    n_select = kwargs['n_select']  # Number of least-count candidates selected per epoch
    n_agents = kwargs['n_agents']
    # All trajectories will be stored in terms of MSM states indices and converted to CV space when required using map_state_to_coordinates
    state_sequences = [[] for _ in range(n_agents)]
    traj_len = kwargs['traj_len']
    delta = kwargs['delta']  # Upper boundary for learning step
    n_features = kwargs['n_features']  # Number of variables in OP space
    max_frames = kwargs['max_frames']
    mapping = kwargs['mapping']
    stakes_method = kwargs['stakes_method']
    stakes_k = kwargs['stakes_k']

    least_counts_points_log = []  # For central frames from candidates
    agent_stakes_log = []
    cumulative_reward_log = [[] for _ in range(n_agents)]
    weights_log = [[] for _ in range(n_agents)]
    individual_rewards_log = [[] for _ in range(n_agents)]
    selected_structures_log = []
    executors_log = []

    # Step 2: set initial weights
    weights = [np.ones((n_features)) / n_features for _ in range(n_agents)]

    # Step 3: collect some initial data
    n_clusters = 0
    while n_clusters < n_select:  # Make sure to have enough states before running REAP
        for a in range(n_agents):
            state_seq = run_monte_carlo_traj(msm, init_state=initial_states[a], n_steps=traj_len)
            state_sequences[a].append(state_seq)
        n_clusters = num_clusters(np.concatenate(np.concatenate(state_sequences)))

    # Step 4: run simulations
    for e in range(epochs):
        print("Running epoch: {}/{}".format(e + 1, epochs), end='\r')

        # Clustering
        kmeans, X, agent_idx = clustering_MA(state_sequences, n_agents, mapping, n_clusters=None, max_frames=max_frames)

        # Select candidates
        central_frames, central_frames_indices, agent_stakes = select_least_counts_MA(kmeans, X, agent_idx, n_agents,
                                                                                      mapping, n_select=n_select,
                                                                                      stakes_method=stakes_method,
                                                                                      stakes_k=stakes_k)

        # Save logs
        least_counts_points_log.append(central_frames)
        agent_stakes_log.append(agent_stakes)

        # Compute rewards and tune weights (this step is done agent-wise)
        for a in range(n_agents):
            # Compute reward and optimize weights
            R, reward_fun = compute_cumulative_reward_MA_standard_Euclidean(X, agent_idx, agent_stakes,
                                                                            central_frames_indices, n_select, n_agents,
                                                                            weights, a, mapping)
            optimization_results = tune_weights_MA_standard_Euclidean(reward_fun, weights[a], delta=delta)

            # Check if optimization worked
            if optimization_results.success:
                pass
            else:
                print("ERROR: CHECK OPTIMIZATION RESULTS FOR AGENT", a, "IN EPOCH", e + 1)
                print(optimization_results)
                break

            # Set new weights
            weights[a] = optimization_results.x
            # Compute individual rewards for storage
            indices = np.where(agent_idx == a)
            X_a = X[indices]
            individual_rewards = compute_structure_reward_MA_standard_Euclidean(mapping(X_a),
                                                                                mapping(central_frames),
                                                                                agent_stakes[a],
                                                                                weights[a],
                                                                                n_select)  # Rewards for each candidate

            # Save logs
            cumulative_reward_log[a].append(R)
            weights_log[a].append(weights[a])
            individual_rewards_log[a].append(individual_rewards)

        chosen_frames, chosen_frames_indices, executors = select_starting_points_MA_standard_Euclidean(X,
                                                                                                       central_frames,
                                                                                                       central_frames_indices,
                                                                                                       agent_idx,
                                                                                                       agent_stakes,
                                                                                                       weights,
                                                                                                       n_agents,
                                                                                                       mapping,
                                                                                                       n_chosen=num_spawn)
        selected_structures_log.append(chosen_frames)
        executors_log.append(executors)

        state_sequences = spawn_trajectories_MA(state_sequences, msm, chosen_frames, executors, traj_len=traj_len)

        if ((e + 1) % 10 == 0):  # Plotting parameters apply for OsSWEET2b example
            print("Running epoch: {}/{}".format(e + 1, epochs))
            for a in range(n_agents):
                print(np.round(weights[a][:2], 2))
                CVs = mapping(np.concatenate(state_sequences[a]))
                plt.scatter(CVs[:, 0], CVs[:, 1])
            plt.xlim([0, 3])
            plt.ylim([0, 2.5])
            plt.savefig(os.path.join(output_dir, output_prefix + 'landscape_epoch_{}.png'.format(e)), dpi=150)
            plt.close()

    # Save results
    save_pickle(state_sequences, os.path.join(output_dir, output_prefix + 'trajectories.pickle'))
    save_pickle(weights_log, os.path.join(output_dir, output_prefix + 'weights_log.pickle'))
    save_pickle(least_counts_points_log, os.path.join(output_dir, output_prefix + 'least_counts_points_log.pickle'))
    save_pickle(cumulative_reward_log, os.path.join(output_dir, output_prefix + 'cumulative_reward_log.pickle'))
    save_pickle(individual_rewards_log, os.path.join(output_dir, output_prefix + 'individual_rewards_log.pickle'))
    save_pickle(selected_structures_log, os.path.join(output_dir, output_prefix + 'selected_structures_log.pickle'))
    save_pickle(agent_stakes_log, os.path.join(output_dir, output_prefix + 'agent_stakes_log.pickle'))
    save_pickle(executors_log, os.path.join(output_dir, output_prefix + 'executors_log.pickle'))
